{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import warnings\n",
    "import network\n",
    "import numpy as np\n",
    "import func\n",
    "import fnmatch\n",
    "from params import dresden_csv, ins_train_csv, ins_test_csv, ins_train, ins_test, \\\n",
    "                ins_patches_db, ins_weights\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving db to csv\n",
      "Saving db to csv\n"
     ]
    }
   ],
   "source": [
    "for path in [ins_train, ins_test]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "data = pd.read_csv(dresden_csv)\n",
    "train_model = ['Ixus70', 'D200', 'mju-1050SW']\n",
    "train_data = data[([m in train_model for m in data['model']])]\n",
    "\n",
    "ins_data = train_data[(train_data['instance']==0)]\n",
    "test_data = train_data[(train_data['instance']!=0)]\n",
    "\n",
    "for db, csv in zip([ins_data, test_data], [ins_train_csv, ins_test_csv]):\n",
    "    brand_model_list = []\n",
    "    for brand, model in zip(db['brand'].values, db['model'].values):\n",
    "        brand_model_list.append('_'.join((brand, model)))\n",
    "\n",
    "    df = pd.DataFrame({'brand_model': brand_model_list, 'path': db['filename'].values})\n",
    "    df.to_csv(csv, index=False)\n",
    "    print('Saving db to csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canon_Ixus70 in training set: 112.\n",
      "Canon_Ixus70 in validation set: 36.\n",
      "Canon_Ixus70 in test set: 39.\n",
      "\n",
      "Nikon_D200 in training set: 239.\n",
      "Nikon_D200 in validation set: 59.\n",
      "Nikon_D200 in test set: 74.\n",
      "\n",
      "Olympus_mju-1050SW in training set: 138.\n",
      "Olympus_mju-1050SW in validation set: 27.\n",
      "Olympus_mju-1050SW in test set: 39.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "images_db = pd.read_csv(ins_train_csv)\n",
    "\n",
    "model_list = np.unique(images_db['brand_model'])\n",
    "img_list = images_db['path']\n",
    "\n",
    "if not os.path.exists(ins_patches_db):\n",
    "    train_list, val_list, test_list, info, weights = func.split(img_list, model_list, ins_patches_db)\n",
    "else:\n",
    "    patches_db = np.load(ins_patches_db, allow_pickle=True).item()\n",
    "    train_list = patches_db['train']\n",
    "    val_list = patches_db['val']\n",
    "    test_list = patches_db['test']\n",
    "    info, weights = func.split_info(train_list, val_list, test_list, \n",
    "                                       model_list, total=len(img_list))\n",
    "\n",
    "# store class weight to csv\n",
    "df = pd.DataFrame([weights], columns=range(len(model_list)))\n",
    "df.to_csv(ins_weights, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112/112 [00:00<00:00, 170450.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance test for camera Canon_Ixus70 has 380 images.\n",
      "Instance test for camera Nikon_D200 has 380 images.\n",
      "Instance test for camera Olympus_mju-1050SW has 836 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 36/36 [00:00<00:00, 20758.17it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 105670.45it/s]\n",
      "100%|██████████| 380/380 [00:00<00:00, 333020.38it/s]\n",
      "100%|██████████| 239/239 [00:00<00:00, 255138.37it/s]\n",
      "100%|██████████| 59/59 [00:00<00:00, 72506.28it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 236208.90it/s]\n",
      "100%|██████████| 380/380 [00:00<00:00, 317118.09it/s]\n",
      "100%|██████████| 138/138 [00:00<00:00, 172163.58it/s]\n",
      "100%|██████████| 27/27 [00:00<00:00, 36204.03it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 36900.04it/s]\n",
      "100%|██████████| 836/836 [00:00<00:00, 483846.85it/s]\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv(ins_test_csv)\n",
    "test_info = []\n",
    "for m in model_list:\n",
    "    tmp = fnmatch.filter(test['path'].values, m + '*')\n",
    "    test_info.append(tmp)\n",
    "    print('Instance test for camera {} has {} images.'.format(m, len(tmp)))\n",
    "\n",
    "for i in range(len(model_list)):\n",
    "    func.patch(model_list[i], info[i][0], 'train', patches_root=ins_train)\n",
    "    func.patch(model_list[i], info[i][1], 'val' , patches_root=ins_train)\n",
    "    func.patch(model_list[i], info[i][2], 'test', patches_root=ins_train)\n",
    "    func.patch(model_list[i], test_info[i], '.', patches_root=ins_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12227 images belonging to 3 classes.\n",
      "Found 3050 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "img_height = 256\n",
    "img_width = 256\n",
    "batch_size = 64\n",
    "\n",
    "train_generator = ImageDataGenerator(preprocessing_function=None,\n",
    "    rescale=1./255, horizontal_flip=True, vertical_flip=True)\n",
    "\n",
    "validation_generator = ImageDataGenerator(preprocessing_function=None,\n",
    "    rescale=1./255)\n",
    "\n",
    "train_data_gen = train_generator.flow_from_directory(\n",
    "    directory=r\"./instance/train/train/\",\n",
    "    target_size=(img_width, img_height), color_mode='grayscale',\n",
    "    batch_size=batch_size, class_mode=\"categorical\", shuffle=True)\n",
    "\n",
    "validation_data_gen = validation_generator.flow_from_directory(\n",
    "    directory=r\"./instance/train/val/\",\n",
    "    target_size=(img_width, img_height), color_mode='grayscale',\n",
    "    batch_size=batch_size, class_mode=\"categorical\", shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training and validation datasets\n",
    "print(\"[*] Define model\")\n",
    "model = network.build()\n",
    "\n",
    "sgd = tf.optimizers.SGD(lr=0.001, momentum=0.9, decay=0.0005)\n",
    "model.compile(\n",
    "    optimizer=sgd, \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy'])\n",
    "\n",
    "#           ------------ Train the Model ------------\n",
    "if not os.path.exists('./instance/saved_model'):\n",
    "    os.makedirs('./instance/saved_model')\n",
    "    \n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "ConstrainLayer = network.ConstrainLayer(model)\n",
    "callbacks = [ModelCheckpoint('./instance/saved_model/weights.{epoch:02d}.h5',\n",
    "    monitor='acc',verbose=1, save_best_only=False,\n",
    "    save_freq=1), ConstrainLayer, tensorboard_callback]\n",
    "\n",
    "df = pd.read_csv(ins_weights)\n",
    "class_weight = df.to_dict('records')[0]\n",
    "class_weight = {int(k):v for k, v in class_weight.items()}\n",
    "\n",
    "history = model.fit_generator(generator=train_data_gen, epochs=45, workers=10,\n",
    "     callbacks=callbacks, validation_data=validation_data_gen, class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./instance/model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
